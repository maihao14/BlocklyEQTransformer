# -*- coding: utf-8 -*-
# MIT License
#
# Copyright (c) 2022 Hao Mai & Pascal Audet
#
# Note that Blockly Earthquake Transformer (BET) is driven by Earthquake Transformer
# V1.59 created by @author: mostafamousavi
# Ref Repo: https://github.com/smousavi05/EQTransformer
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import print_function
from keras import backend as K
from keras.models import load_model
from keras.optimizers import Adam
# import os
# os.environ['KERAS_BACKEND']='tensorflow'
# from tensorflow.keras import backend as K
# from tensorflow.keras.models import load_model
# from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
import numpy as np
import csv
import h5py
import time
import os
import shutil
import pandas as pd
from .EqT_utils import f1, SeqSelfAttention, FeedForward, LayerNormalization
from .EqT_utils import generate_arrays_from_file, picker
from .EqT_utils import DataGeneratorTest, PreLoadGeneratorTest
np.warnings.filterwarnings('ignore')
import datetime
from tqdm import tqdm
from tensorflow.python.util import deprecation
deprecation._PRINT_DEPRECATION_WARNINGS = False


def tester(input_hdf5=None,
           input_testset=None,
           input_model=None,
           output_name=None,
           detection_threshold=0.20,
           P_threshold=0.1,
           S_threshold=0.1,
           number_of_plots=100,
           estimate_uncertainty=False,
           number_of_sampling=5,
           loss_weights=[0.05, 0.40, 0.55],
           loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],
           input_dimention=(6000, 3),
           normalization_mode='std',
           mode='generator',
           batch_size=500,
           gpuid=None,
           gpu_limit=None,
           phase_types=['d', 'P', 'S']): #default phase types

    """

    Applies a trained model to a windowed waveform to perform both detection and picking at the same time.


    Parameters
    ----------
    input_hdf5: str, default=None
        Path to an hdf5 file containing only one class of "data" with NumPy arrays containing 3 component waveforms each 1 min long.

    input_testset: npy, default=None
        Path to a NumPy file (automaticaly generated by the trainer) containing a list of trace names.

    input_model: str, default=None
        Path to a trained model.

    output_dir: str, default=None
        Output directory that will be generated.

    output_probabilities: bool, default=False
        If True, it will output probabilities and estimated uncertainties for each trace into an HDF file.

    detection_threshold : float, default=0.3
        A value in which the detection probabilities above it will be considered as an event.

    P_threshold: float, default=0.1
        A value which the P probabilities above it will be considered as P arrival.

    S_threshold: float, default=0.1
        A value which the S probabilities above it will be considered as S arrival.

    number_of_plots: float, default=10
        The number of plots for detected events outputed for each station data.

    estimate_uncertainty: bool, default=False
        If True uncertainties in the output probabilities will be estimated.

    number_of_sampling: int, default=5
        Number of sampling for the uncertainty estimation.

    loss_weights: list, default=[0.03, 0.40, 0.58]
        Loss weights for detection, P picking, and S picking respectively.

    loss_types: list, default=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy']
        Loss types for detection, P picking, and S picking respectively.

    input_dimention: tuple, default=(6000, 3)
        Loss types for detection, P picking, and S picking respectively.

    normalization_mode: str, default='std'
        Mode of normalization for data preprocessing, 'max', maximum amplitude among three components, 'std', standard deviation.

    mode: str, default='generator'
        Mode of running. 'pre_load_generator' or 'generator'.

    batch_size: int, default=500
        Batch size. This wont affect the speed much but can affect the performance. A value beteen 200 to 1000 is recommanded.

    gpuid: int, default=None
        Id of GPU used for the prediction. If using CPU set to None.

    gpu_limit: int, default=None
        Set the maximum percentage of memory usage for the GPU.


    Returns
    --------
    ./output_name/X_test_results.csv: A table containing all the detection, and picking results. Duplicated events are already removed.

    ./output_name/X_report.txt: A summary of the parameters used for prediction and performance.

    ./output_name/figures: A folder containing plots detected events and picked arrival times.


    Notes
    --------
    Estimating the uncertainties requires multiple predictions and will increase the computational time.


    """


    args = {
    "input_hdf5": input_hdf5,
    "input_testset": input_testset,
    "input_model": input_model,
    "output_name": output_name,
    "detection_threshold": detection_threshold,
    "P_threshold": P_threshold,
    "S_threshold": S_threshold,
    "number_of_plots": number_of_plots,
    "estimate_uncertainty": estimate_uncertainty,
    "number_of_sampling": number_of_sampling,
    "loss_weights": loss_weights,
    "loss_types": loss_types,
    "input_dimention": input_dimention,
    "normalization_mode": normalization_mode,
    "mode": mode,
    "batch_size": batch_size,
    "gpuid": gpuid,
    "gpu_limit": gpu_limit,
    "phase_types": phase_types
    }


    if args['gpuid']:
        os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(args['gpuid'])
        tf.Session(config=tf.ConfigProto(log_device_placement=True))
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit'])
        K.tensorflow_backend.set_session(tf.Session(config=config))

    save_dir = os.path.join(os.getcwd(), str(args['output_name'])+'_outputs')
    save_figs = os.path.join(save_dir, 'figures')

    if os.path.isdir(save_dir):
        shutil.rmtree(save_dir)
    os.makedirs(save_figs)

    test = np.load(args['input_testset'])

    print('Loading the model ...', flush=True)
    model = load_model(args['input_model'], custom_objects={'SeqSelfAttention': SeqSelfAttention,
                                                         'FeedForward': FeedForward,
                                                         'LayerNormalization': LayerNormalization,
                                                         'f1': f1
                                                         })

    model.compile(loss = args['loss_types'],
                  loss_weights =  args['loss_weights'],
                  optimizer = Adam(lr = 0.001),
                  metrics = [f1])

    print('Loading is complete!', flush=True)
    print('Testing ...', flush=True)
    print('Writting results into: " ' + str(args['output_name'])+'_outputs'+' "', flush=True)

    start_training = time.time()

    csvTst = open(os.path.join(save_dir,'X_test_results.csv'), 'w')
    test_writer = csv.writer(csvTst, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    test_writer.writerow(['network_code',
                          'ID',
                          'earthquake_distance_km',
                          'snr_db',
                          'trace_name',
                          'trace_category',
                          'trace_start_time',
                          'source_magnitude',
                          'arrivals',
                          'receiver_type',

                          'number_of_detections',
                          'detection_probability',
                          'detection_uncertainty',

                          'P_pick',
                          'P_probability',
                          'P_uncertainty',
                          'P_error',
                          #Pg
                          'Pg_pick',
                          'Pg_probability',
                          'Pg_uncertainty',
                          'Pg_error',
                           #Pn
                           'Pn_pick',
                           'Pn_probability',
                           'Pn_uncertainty',
                           'Pn_error',

                          'S_pick',
                          'S_probability',
                          'S_uncertainty',
                          'S_error',
                          # Sg
                          'Sg_pick',
                          'Sg_probability',
                          'Sg_uncertainty',
                          'Sg_error',
                          # Sn
                          'Sn_pick',
                          'Sn_probability',
                          'Sn_uncertainty',
                          'Sn_error'
                          ])
    csvTst.flush()

    df = pd.DataFrame(columns=['network_code',
                          'ID',
                          #'earthquake_distance_km',
                          #'snr_db',
                          'trace_name',
                          'trace_category',
                          'trace_start_time',
                          'source_magnitude',
                          #'p_arrival_sample',
                          #'p_status',
                          #'p_weight',
                          #'s_arrival_sample',
                          #'s_status',
                          #'s_weight',
                          #'receiver_type',

                          #'number_of_detections',
                          #'detection_probability',
                          #'detection_uncertainty',

                          #'P_pick',
                          #'P_probability',
                          #'P_uncertainty',
                          #'P_error',

                          #'S_pick',
                          #'S_probability',
                          #'S_uncertainty',
                          #'S_error'
                          ])
    plt_n = 0
    list_generator = generate_arrays_from_file(test, args['batch_size'])

    pbar_test = tqdm(total= int(np.ceil(len(test)/args['batch_size'])))
    for _ in range(int(np.ceil(len(test) / args['batch_size']))):
        pbar_test.update()
        new_list = next(list_generator)

        if args['mode'].lower() == 'pre_load_generator':
            params_test = {'dim': args['input_dimention'][0],
                           'batch_size': len(new_list),
                           'n_channels': args['input_dimention'][-1],
                           'norm_mode': args['normalization_mode']}
            test_set={}
            fl = h5py.File(args['input_hdf5'], 'r')
            for ID in new_list:
                if ID.split('_')[-1] != 'NO':
                    dataset = fl.get('data/'+str(ID))
                elif ID.split('_')[-1] == 'NO':
                    dataset = fl.get('data/'+str(ID))
                test_set.update( {str(ID) : dataset})

            test_generator = PreLoadGeneratorTest(new_list, test_set, **params_test)

            if args['estimate_uncertainty']:
                pred_DD = []
                pred_PP = []
                pred_SS = []
                for mc in range(args['number_of_sampling']):
                    predD, predP, predS = model.predict_generator(test_generator)
                    pred_DD.append(predD)
                    pred_PP.append(predP)
                    pred_SS.append(predS)

                pred_DD = np.array(pred_DD).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])
                pred_DD_mean = pred_DD.mean(axis=0)
                pred_DD_std = pred_DD.std(axis=0)

                pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])
                pred_PP_mean = pred_PP.mean(axis=0)
                pred_PP_std = pred_PP.std(axis=0)

                pred_SS = np.array(pred_SS).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])
                pred_SS_mean = pred_SS.mean(axis=0)
                pred_SS_std = pred_SS.std(axis=0)

            else:
                pred_DD_mean, pred_PP_mean, pred_SS_mean = model.predict_generator(test_generator)
                pred_DD_mean = pred_DD_mean.reshape(pred_DD_mean.shape[0], pred_DD_mean.shape[1])
                pred_PP_mean = pred_PP_mean.reshape(pred_PP_mean.shape[0], pred_PP_mean.shape[1])
                pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1])

                pred_DD_std = np.zeros((pred_DD_mean.shape))
                pred_PP_std = np.zeros((pred_PP_mean.shape))
                pred_SS_std = np.zeros((pred_SS_mean.shape))

            for ts in range(pred_DD_mean.shape[0]):
                evi =  new_list[ts]
                dataset = test_set[evi]

                try:
                    spt = int(dataset.attrs['p_arrival_sample']);
                except Exception:
                    spt = None

                try:
                    sst = int(dataset.attrs['s_arrival_sample']);
                except Exception:
                    sst = None

                matches, pick_errors, yh3 =  picker(args, pred_DD_mean[ts], pred_PP_mean[ts], pred_SS_mean[ts],
                                                    pred_DD_std[ts], pred_PP_std[ts], pred_SS_std[ts], spt, sst)

                _output_writter_test(args, dataset, evi, test_writer, csvTst, matches, pick_errors)

                if plt_n < args['number_of_plots']:

                    _plotter(ts,
                            dataset,
                            evi,
                            args,
                            save_figs,
                            pred_DD_mean[ts],
                            pred_PP_mean[ts],
                            pred_SS_mean[ts],
                            pred_DD_std[ts],
                            pred_PP_std[ts],
                            pred_SS_std[ts],
                            matches)

                plt_n += 1

        # Generator mode
        else:
            params_test = {'file_name': str(args['input_hdf5']),
                           'dim': args['input_dimention'][0],
                           'batch_size': len(new_list),
                           'n_channels': args['input_dimention'][-1],
                           'norm_mode': args['normalization_mode']}

            test_generator = DataGeneratorTest(new_list, **params_test)
            # uncertainty estimation
            if args['estimate_uncertainty']:
                pred_DD = []
                pred_PP = []
                pred_SS = []
                for mc in range(args['number_of_sampling']):
                    predD, predP, predS = model.predict_generator(generator=test_generator)
                    pred_DD.append(predD)
                    pred_PP.append(predP)
                    pred_SS.append(predS)

                pred_DD = np.array(pred_DD).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])
                pred_DD_mean = pred_DD.mean(axis=0)
                pred_DD_std = pred_DD.std(axis=0)

                pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])
                pred_PP_mean = pred_PP.mean(axis=0)
                pred_PP_std = pred_PP.std(axis=0)

                pred_SS = np.array(pred_SS).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])
                pred_SS_mean = pred_SS.mean(axis=0)
                pred_SS_std = pred_SS.std(axis=0)
            # Use Simple Prediction Mode
            else:
                pred_DD = model.predict_generator(generator=test_generator)
                index = 0
                prob_dic = dict()
                if 'd' in args["phase_types"] or 'D' in args["phase_types"] or 'Detector' in args["phase_types"]:
                    # add detector prediction
                    pred_DD_mean = pred_DD[index]
                    index = index + 1
                    pred_DD_mean = pred_DD_mean.reshape(pred_DD_mean.shape[0], pred_DD_mean.shape[1])
                    pred_DD_std = np.zeros((pred_DD_mean.shape))
                    prob_dic['DD_mean'] = pred_DD_mean
                    prob_dic['DD_std'] = pred_DD_std
                for picker_name in args["phase_types"]:
                    if picker_name == 'P':
                        # add P-type output prediction
                        pred_PP_mean = pred_DD[index]
                        pred_PP_mean = pred_PP_mean.reshape(pred_PP_mean.shape[0], pred_PP_mean.shape[1])
                        pred_PP_std = np.zeros((pred_PP_mean.shape))
                        prob_dic['PP_mean'] = pred_PP_mean
                        prob_dic['PP_std'] = pred_PP_std
                        index = index + 1
                    if picker_name == 'S':
                        # add S-type output prediction
                        pred_SS_mean = pred_DD[index]
                        pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1])
                        pred_SS_std = np.zeros((pred_SS_mean.shape))
                        prob_dic['SS_mean'] = pred_SS_mean
                        prob_dic['SS_std'] = pred_SS_std
                        index = index + 1
                    if picker_name == 'Pn':
                        # add Pn-type output prediction
                        pred_PN_mean = pred_DD[index]
                        pred_PN_mean = pred_PN_mean.reshape(pred_PN_mean.shape[0], pred_PN_mean.shape[1])
                        pred_PN_std = np.zeros((pred_PN_mean.shape))
                        prob_dic['PN_mean'] = pred_PN_mean
                        prob_dic['PP_std'] = pred_PN_std
                        index = index + 1
                    if picker_name == 'Sn':
                        # add Sn-type output channel
                        pred_SN_mean = pred_DD[index]
                        pred_SN_mean = pred_SN_mean.reshape(pred_SN_mean.shape[0], pred_SN_mean.shape[1])
                        pred_SN_std = np.zeros((pred_SN_mean.shape))
                        prob_dic['SN_mean'] = pred_SN_mean
                        prob_dic['SS_std'] = pred_SN_std
                        index = index + 1
                    if picker_name == 'Pg':
                        # add Pg-type output channel
                        pred_PG_mean = pred_DD[index]
                        pred_PG_mean = pred_PG_mean.reshape(pred_PG_mean.shape[0], pred_PG_mean.shape[1])
                        pred_PG_std = np.zeros((pred_PG_mean.shape))
                        prob_dic['PG_mean'] = pred_PG_mean
                        prob_dic['PG_std'] = pred_PG_std
                        index = index + 1
                    if picker_name == 'Sg':
                        # add Sg-type output channel
                        pred_SG_mean = pred_DD[index]
                        pred_SG_mean = pred_SG_mean.reshape(pred_SG_mean.shape[0], pred_SG_mean.shape[1])
                        pred_SG_std = np.zeros((pred_SG_mean.shape))
                        prob_dic['SG_mean'] = pred_SG_mean
                        prob_dic['SG_std'] = pred_SG_std
                        index = index + 1
                #pred_DD_mean, pred_PP_mean, pred_SS_mean = model.predict_generator(generator=test_generator)
                # pred_DD_mean = pred_DD_mean.reshape(pred_DD_mean.shape[0], pred_DD_mean.shape[1])
                # pred_PP_mean = pred_PP_mean.reshape(pred_PP_mean.shape[0], pred_PP_mean.shape[1])
                # pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1])
                #
                # pred_DD_std = np.zeros((pred_DD_mean.shape))
                # pred_PP_std = np.zeros((pred_PP_mean.shape))
                # pred_SS_std = np.zeros((pred_SS_mean.shape))

            test_set={}
            fl = h5py.File(args['input_hdf5'], 'r')
            for ID in new_list:
                if ID.split('_')[-1] != 'NO':
                    dataset = fl.get('data/'+str(ID))
                elif ID.split('_')[-1] == 'NO':
                    dataset = fl.get('data/'+str(ID))
                test_set.update( {str(ID) : dataset})
            # global matches
            # global matches2
            # global matches3
            matches = {}
            matches2 = {}
            matches3 = {}
            global pick_errors2
            global pick_errors3
            keys = []
            for key in prob_dic.keys():
                keys.append(key)
            if 'DD_mean' not in keys:
                prob_dic['DD_mean'] = {}
            if 'PP_mean' not in keys:
                prob_dic['PP_mean'] = {}
            if 'SS_mean' not in keys:
                prob_dic['SS_mean'] = {}
            if 'PN_mean' not in keys:
                prob_dic['PN_mean'] = {}
            if 'SN_mean' not in keys:
                prob_dic['SN_mean'] = {}
            if 'PG_mean' not in keys:
                prob_dic['PG_mean'] = {}
            if 'SG_mean' not in keys:
                prob_dic['SG_mean'] = {}

            for ts in range(prob_dic[keys[0]].shape[0]):
                evi =  new_list[ts]
                dataset = test_set[evi]
                # matches, pick_errors, yh3=picker(args, pred_DD_mean[ts], pred_PP_mean[ts], pred_SS_mean[ts],
                #                                        pred_DD_std[ts], pred_PP_std[ts], pred_SS_std[ts], spt, sst)
                if 'DD_mean' in keys and 'PP_mean' in keys and 'SS_mean' in keys:
                    # Load P, S arrival time from new STEAD format
                    try:
                        arrivals = dataset.attrs['p_pn_pg_s_sn_sg']
                    except:
                        arrivals = np.array([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
                    # Load P, S arrival time from original STEAD format
                    try:
                        arrivals[0] = int(dataset.attrs['p_arrival_sample'])
                        arrivals[3] = int(dataset.attrs['s_arrival_sample'])
                    except:
                        pass
                    spt = arrivals[0]
                    sst = arrivals[3]
                    matches, pick_errors, yh3 = picker(args, pred_DD_mean[ts], pred_PP_mean[ts], pred_SS_mean[ts],
                                                       pred_DD_std[ts], pred_PP_std[ts], pred_SS_std[ts], spt, sst)
                else:
                    prob_dic['PP_mean'][ts] = None
                    prob_dic['SS_mean'][ts] = None
                    spt = None
                    sst = None
                    pick_errors = None


                if 'DD_mean' in keys and 'PN_mean' in keys and 'SN_mean' in keys:
                    # Load Pn, Sn arrival time from new STEAD format
                    try:
                        arrivals = dataset.attrs['p_pn_pg_s_sn_sg']
                    except:
                        arrivals = np.array([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
                    spt2 = arrivals[1]
                    sst2 = arrivals[4]
                    matches2, pick_errors2, yh3 = picker(args, pred_DD_mean[ts], pred_PN_mean[ts], pred_SN_mean[ts],
                                                       pred_DD_std[ts], pred_PN_std[ts], pred_SN_std[ts], spt2, sst2)
                else:
                    prob_dic['PN_mean'][ts] = None
                    prob_dic['SN_mean'][ts] = None
                    spt2 = None
                    sst2 = None
                    pick_errors2 = None

                if 'DD_mean' in keys and 'PG_mean' in keys and 'SG_mean' in keys:
                    # Load Pg, Sg arrival time from new STEAD format
                    try:
                        arrivals = dataset.attrs['p_pn_pg_s_sn_sg']
                    except:
                        arrivals = np.array([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
                    spt3 = arrivals[2]
                    sst3 = arrivals[5]
                    matches3, pick_errors3, yh3 = picker(args, pred_DD_mean[ts], pred_PG_mean[ts], pred_SG_mean[ts],
                                                       pred_DD_std[ts], pred_PG_std[ts], pred_SG_std[ts], spt3, sst3)
                else:
                    prob_dic['PG_mean'][ts] = None
                    prob_dic['SG_mean'][ts] = None
                    spt3 = None
                    sst3 = None
                    pick_errors3 = None
                if plt_n < args['number_of_plots'] and (matches or matches2 or matches3):
                    dat = np.array(dataset)
                    # Hao update Nov 6 2022
                    # convert data format to the one that is used in the prediction
                    if dat.shape[0] <= 10:  # assume the original shape is (n_channels, n_samples )
                        dat = np.transpose(dat)
                    # check data shape e.g, sample length < required n_samples, start duplicating
                    if dat.shape[0] < args["input_dimention"][0]:
                        duplicate_len = int(args["input_dimention"][0] - dat.shape[0])
                        dat = np.concatenate((dat, dat[0:duplicate_len, :]))
                    else:
                        # check data shape e.g, sample length > required n_samples, start trimming
                        if dat.shape[0] > args["input_dimention"][0]:
                            dat = dat[0:args["input_dimention"][0], :]
                    # Hao update Nov 6 2022
                    # augment when trace channel is less than required n_channels
                    if dat.ndim == 1:
                        dat_channel = 1
                    else:
                        dat_channel = dat.shape[1]
                    if dat_channel < args["input_dimention"][1]:
                        temp = dat
                        dat = np.zeros((args["input_dimention"][0], args["input_dimention"][1]))
                        if dat_channel == 1:
                            dat[:, 0] = temp.flatten()
                        else:
                            dat[:, 0:dat_channel] = temp
                        if dat_channel < args["input_dimention"][1]:
                            for i in range(dat_channel, temp.shape[1]):
                                dat[:, i] = dat[:, 0]

                    _plotter_mul_prediction(dat, evi, args, save_figs, matches, keys, matches2, matches3,
                                        prob_dic['DD_mean'][ts],
                                        prob_dic['PP_mean'][ts],
                                        prob_dic['SS_mean'][ts],
                                        prob_dic['PN_mean'][ts],
                                        prob_dic['SN_mean'][ts],
                                        prob_dic['PG_mean'][ts],
                                        prob_dic['SG_mean'][ts],
                                        spt, sst, spt2, sst2, spt3, sst3)
                    _output_writter_test(args, dataset, evi, test_writer, csvTst, matches, pick_errors, matches2,
                                         pick_errors2, matches3, pick_errors3)
                    plt_n += 1

                # if plt_n < args['number_of_plots']:
                #
                #     _plotter(dataset,
                #                 evi,
                #                 args,
                #                 save_figs,
                #                 pred_DD_mean[ts],
                #                 pred_PP_mean[ts],
                #                 pred_SS_mean[ts],
                #                 pred_DD_std[ts],
                #                 pred_PP_std[ts],
                #                 pred_SS_std[ts],
                #                 matches)
                #
                # plt_n += 1
    end_training = time.time()
    delta = end_training - start_training
    hour = int(delta / 3600)
    delta -= hour * 3600
    minute = int(delta / 60)
    delta -= minute * 60
    seconds = delta

    with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file:
        the_file.write('================== Overal Info =============================='+'\n')
        the_file.write('date of report: '+str(datetime.datetime.now())+'\n')
        the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\n')
        the_file.write('input_testset: '+str(args['input_testset'])+'\n')
        the_file.write('input_model: '+str(args['input_model'])+'\n')
        the_file.write('output_name: '+str(args['output_name']+'_outputs')+'\n')
        the_file.write('================== Testing Parameters ======================='+'\n')
        the_file.write('mode: '+str(args['mode'])+'\n')
        the_file.write('finished the test in:  {} hours and {} minutes and {} seconds \n'.format(hour, minute, round(seconds, 2)))
        the_file.write('loss_types: '+str(args['loss_types'])+'\n')
        the_file.write('loss_weights: '+str(args['loss_weights'])+'\n')
        the_file.write('batch_size: '+str(args['batch_size'])+'\n')
        the_file.write('total number of tests '+str(len(test))+'\n')
        the_file.write('gpuid: '+str(args['gpuid'])+'\n')
        the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\n')
        the_file.write('================== Other Parameters ========================='+'\n')
        the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\n')
        the_file.write('estimate uncertainty: '+str(args['estimate_uncertainty'])+'\n')
        the_file.write('number of Monte Carlo sampling: '+str(args['number_of_sampling'])+'\n')
        the_file.write('detection_threshold: '+str(args['detection_threshold'])+'\n')
        the_file.write('P_threshold: '+str(args['P_threshold'])+'\n')
        the_file.write('S_threshold: '+str(args['S_threshold'])+'\n')
        the_file.write('number_of_plots: '+str(args['number_of_plots'])+'\n')





def _output_writter_test(args,
                        dataset,
                        evi,
                        output_writer,
                        csvfile,
                        matches,
                        pick_errors,
                        matches2,
                        pick_errors2,
                        matches3,
                        pick_errors3
                        ):

    """

    Writes the detection & picking results into a CSV file.

    Parameters
    ----------
    args: dic
        A dictionary containing all of the input parameters.

    dataset: hdf5 obj
        Dataset object of the trace.

    evi: str
        Trace name.

    output_writer: obj
        For writing out the detection/picking results in the CSV file.

    csvfile: obj
        For writing out the detection/picking results in the CSV file.

    matches: dic
        Contains the information for the detected and picked event.

    pick_errors: dic
        Contains prediction errors for P and S picks.

    Returns
    --------
    X_test_results.csv


    """


    numberOFdetections = len(matches)

    if numberOFdetections != 0:
        D_prob =  matches[list(matches)[0]][1]
        D_unc = matches[list(matches)[0]][2]

        P_arrival = matches[list(matches)[0]][3]
        P_prob = matches[list(matches)[0]][4]
        P_unc = matches[list(matches)[0]][5]
        P_error = pick_errors[list(matches)[0]][0]

        S_arrival = matches[list(matches)[0]][6]
        S_prob = matches[list(matches)[0]][7]
        S_unc = matches[list(matches)[0]][8]
        S_error = pick_errors[list(matches)[0]][1]

    else:
        D_prob = None
        D_unc = None

        P_arrival = None
        P_prob = None
        P_unc = None
        P_error = None

        S_arrival = None
        S_prob = None
        S_unc = None
        S_error = None
    numberOFdetections2 = len(matches2)
    if numberOFdetections2 != 0:
        D_prob2 =  matches2[list(matches2)[0]][1]
        D_unc2 = matches2[list(matches2)[0]][2]

        Pg_arrival2 = matches2[list(matches2)[0]][3]
        Pg_prob2 = matches2[list(matches2)[0]][4]
        Pg_unc2 = matches2[list(matches2)[0]][5]
        Pg_error2 = pick_errors2[list(matches2)[0]][0]

        Sg_arrival2 = matches2[list(matches2)[0]][6]
        Sg_prob2 = matches2[list(matches2)[0]][7]
        Sg_unc2 = matches2[list(matches2)[0]][8]
        Sg_error2 = pick_errors2[list(matches2)[0]][1]
    else:
        D_prob2 = None
        D_unc2 = None

        Pg_arrival2 = None
        Pg_prob2 = None
        Pg_unc2 = None
        Pg_error2 = None

        Sg_arrival2 = None
        Sg_prob2 = None
        Sg_unc2 = None
        Sg_error2 = None
    numberOFdetections3 = len(matches3)
    if numberOFdetections3 != 0:
        D_prob3 =  matches3[list(matches3)[0]][1]
        D_unc3 = matches3[list(matches3)[0]][2]

        Pn_arrival3 = matches3[list(matches3)[0]][3]
        Pn_prob3 = matches3[list(matches3)[0]][4]
        Pn_unc3 = matches3[list(matches3)[0]][5]
        Pn_error3 = pick_errors3[list(matches3)[0]][0]

        Sn_arrival3 = matches3[list(matches3)[0]][6]
        Sn_prob3 = matches3[list(matches3)[0]][7]
        Sn_unc3 = matches3[list(matches3)[0]][8]
        Sn_error3 = pick_errors3[list(matches3)[0]][1]
    else:
        D_prob3 = None
        D_unc3 = None

        Pn_arrival3 = None
        Pn_prob3 = None
        Pn_unc3 = None
        Pn_error3 = None

        Sn_arrival3 = None
        Sn_prob3 = None
        Sn_unc3 = None
        Sn_error3 = None
    trace_name = dataset.attrs["trace_name"]
    stainfo = trace_name.split('_')[0]
    station_name = stainfo.split('.')[0]
    network_code = stainfo.split('.')[-1]
    receiver_type = trace_name.split('_')[2]
    receiver_type = "{:<2}".format(receiver_type)
    source_id = None
    source_distance_km = None
    trace_category = trace_name.split('_')[-1]
    source_magnitude = -1
    p_status = 'manual'
    p_weight = None
    s_status = 'manual'
    s_weight = None
    snr_db = None
    try:
        snr_db = np.mean(dataset.attrs['snr_db'])
        trace_category = dataset.attrs['trace_category']
        source_magnitude = float(dataset.attrs['source_magnitude'])
    except:
        pass
    try:
        trace_start_time = dataset.attrs['trace_start_time']
    except Exception:
        trace_start_time = trace_name.split('_')[1]
        trace_start_time = trace_start_time[:14]
    if evi.split('_')[-1] == 'EV':
        try:
            arrivals = dataset.attrs['p_pn_pg_s_sn_sg']
        except:
            arrivals = np.array([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
        # Load P, S arrival time from original STEAD format
        try:
            arrivals[0] = int(dataset.attrs['p_arrival_sample'])
            arrivals[3] = int(dataset.attrs['s_arrival_sample'])
        except:
            pass

    if evi.split('_')[-1] == 'NO':
        arrivals = np.array([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
        p_arrival_sample = None
        p_status = 'manual'
        p_weight = None
        s_arrival_sample = None
        s_status = None
        s_weight = None


    if P_unc:
        P_unc = round(P_unc, 3)



    output_writer.writerow([network_code,
                            source_id,
                            source_distance_km,
                            snr_db,
                            trace_name,
                            trace_category,
                            trace_start_time,
                            source_magnitude,
                            arrivals,
                            p_status,
                            p_weight,
                            s_status,
                            s_weight,
                            receiver_type,

                            numberOFdetections,
                            D_prob,
                            D_unc,

                            P_arrival,
                            P_prob,
                            P_unc,
                            P_error,

                            S_arrival,
                            S_prob,
                            S_unc,
                            S_error,

                            Pg_arrival2,
                            Pg_prob2,
                            Pg_unc2,
                            Pg_error2,

                            Sg_arrival2,
                            Sg_prob2,
                            Sg_unc2,
                            Sg_error2,

                            Pn_arrival3,
                            Pn_prob3,
                            Pn_unc3,
                            Pn_error3,

                            Sn_arrival3,
                            Sn_prob3,
                            Sn_unc3,
                            Sn_error3

                            ])

    csvfile.flush()




def _plotter_mul_prediction(data, evi, args, save_figs, matches, keys, matches2= None, matches3= None, yh1 = None,
                            yh2 = None, yh3=None, yh4=None, yh5=None, yh6=None, yh7=None,
                            p=None, s=None, p2=None, s2=None, p3=None, s3=None):
    """
    Adaptively generates plots of detected events waveforms, output predictions, and picked arrival times.

    Parameters
    ----------
    data: NumPy array
        N component raw waveform.

    evi : str
        Trace name.

    args: dic
        A dictionary containing all of the input parameters.

    save_figs: str
        Path to the folder for saving the plots.

    matches: dic
        Contains the information for the P and S detected and picked event.

    matches2: dic
        Contains the information for the P and S detected and picked event.

    matches3: dic
        Contains the information for the P and S detected and picked event.

    yh1: 1D array
        Detection probabilities.

    yh2: 1D array
        P arrival probabilities.

    yh3: 1D array
        S arrival probabilities.

    yh4: 1D array
        Pn arrival probabilities.

    yh5: 1D array
        Sn arrival probabilities.

    yh6: 1D array
        Pg arrival probabilities.

    yh7: 1D array
        Sg arrival probabilities.


    """
    #fetching detector and P and S picker
    if matches:
        spt, sst, detected_events = [], [], []
        for match, match_value in matches.items():
            detected_events.append([match, match_value[0]])
            if match_value[3]:
                spt.append(match_value[3])
            else:
                spt.append(None)

            if match_value[6]:
                sst.append(match_value[6])
            else:
                sst.append(None)
    if matches2:
        spt2, sst2, detected_events2 = [], [], []
        for match, match_value in matches2.items():
            detected_events2.append([match, match_value[0]])
            if match_value[3]:
                spt2.append(match_value[3])
            else:
                spt2.append(None)

            if match_value[6]:
                sst2.append(match_value[6])
            else:
                sst2.append(None)
    if matches3:
        spt3, sst3, detected_events3 = [], [], []
        for match, match_value in matches3.items():
            detected_events3.append([match, match_value[0]])
            if match_value[3]:
                spt3.append(match_value[3])
            else:
                spt3.append(None)

            if match_value[6]:
                sst3.append(match_value[6])
            else:
                sst3.append(None)
    if data.ndim == 1:
        dat_channel = 1
    else:
        dat_channel = data.shape[1]
    fig = plt.figure()
    fig_num = dat_channel + 1
    for i in range(fig_num-1):
        # plot the n-component raw data
        ax = fig.add_subplot(fig_num, 1, i+1)
        plt.plot(data[:, i], 'k')
        ymin, ymax = ax.get_ylim()
        ground_truth_min = ymin
        ground_truth_max = ymax
        ymin = int(ymin*0.8)
        ymax = int(ymax*0.8)
        # plotting the detected P and S events
        if matches:
            pl = sl = None
            if not p!= p:
                plp = plt.vlines(int(p), ground_truth_min, ground_truth_max, color='b', linewidth=2, label='Manual-Picked P')
            if not s!= s:
                slp = plt.vlines(int(s), ground_truth_min, ground_truth_max, color='r', linewidth=2, label='Manual-Picked S')
            if len(spt) > 0 and np.count_nonzero(data[:, 0]) > 10:
                ymin, ymax = ax.get_ylim()
                ground_truth_min = ymin
                ground_truth_max = ymax
                ymin = int(ymin * 0.8)
                ymax = int(ymax * 0.8)
                for ipt, pt in enumerate(spt):
                    if pt and ipt == 0:
                        pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=1.5, label='Picked P')
                    elif pt and ipt > 0:
                        pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=1.5)

            if len(sst) > 0 and np.count_nonzero(data[:, 0]) > 10:
                for ist, st in enumerate(sst):
                    if st and ist == 0:
                        sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=1.5, label='Picked S')
                    elif st and ist > 0:
                        sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=1.5)
        if matches2:
            pl2 = sl2 = None
            plp2 = plt.vlines(int(p2), ground_truth_min, ground_truth_max, color='cadetblue', linewidth=2, label='Manual-Picked Pn')
            slp2 = plt.vlines(int(s2), ground_truth_min, ground_truth_max, color='tomato', linewidth=2, label='Manual-Picked Sn')
            if len(spt2) > 0 and np.count_nonzero(data[:, 0]) > 10:
                for ipt, pt in enumerate(spt2):
                    if pt and ipt == 0:
                        pl2 = plt.vlines(int(pt), ymin, ymax, color='cyan', linewidth=1.5, label='Picked Pn')
                    elif pt and ipt > 0:
                        pl2 = plt.vlines(int(pt), ymin, ymax, color='cyan', linewidth=1.5)
            if len(sst2) > 0 and np.count_nonzero(data[:, 0]) > 10:
                for ist, st in enumerate(sst2):
                    if st and ist == 0:
                        sl2 = plt.vlines(int(st), ymin, ymax, color='m', linewidth=1.5, label='Picked Sn')
                    elif st and ist > 0:
                        sl2 = plt.vlines(int(st), ymin, ymax, color='fuchsia', linewidth=1.5)
        if matches3:
            pl3 = sl3 = None
            plp3 = plt.vlines(int(p3), ground_truth_min, ground_truth_max, color='steelblue', linewidth=2, label='Manual-Picked Pg')
            slp3 = plt.vlines(int(s3), ground_truth_min, ground_truth_max, color='sienna', linewidth=2, label='Manual-Picked Sg')
            if len(spt3) > 0 and np.count_nonzero(data[:, 0]) > 10:
                ymin, ymax = ax.get_ylim()
                for ipt, pt in enumerate(spt3):
                    if pt and ipt == 0:
                        pl3 = plt.vlines(int(pt), ymin, ymax, color='dodgerblue', linewidth=1.5, label='Picked Pg')
                    elif pt and ipt > 0:
                        pl3 = plt.vlines(int(pt), ymin, ymax, color='dodgerblue', linewidth=1.5)

            if len(sst3) > 0 and np.count_nonzero(data[:, 0]) > 10:
                for ist, st in enumerate(sst3):
                    if st and ist == 0:
                        sl3 = plt.vlines(int(st), ymin, ymax, color='crimson', linewidth=1.5, label='Picked Sg')
                    elif st and ist > 0:
                        sl3 = plt.vlines(int(st), ymin, ymax, color='crimson', linewidth=1.5)

        plt.rcParams["figure.figsize"] = (16, 9)
        #plt.text(0, 10000, "E", fontsize=16)
        if i ==0:
            plt.title('Trace Name: ' +str(evi), fontsize=16)
        plt.tight_layout()
        plt.legend(loc='upper right', borderaxespad=0., fontsize=16)
        plt.ylabel('Amplitude\nCounts', fontsize=16)
    # plot the detection results
    i = i+1
    ax = fig.add_subplot(fig_num, 1, i + 1)
    # plotting the detected P and S events
    if matches:
        plt.plot(yh2, '--', color='deepskyblue', alpha=0.5, linewidth=1.5, label='P  Prediction')
        plt.plot(yh3, '--', color='deeppink', alpha=0.5, linewidth=1.5, label='S  Prediction')
    if matches2:
        plt.plot(yh4, '--', color='cyan', alpha=0.5, linewidth=1.5, label='Pn Prediction')
        plt.plot(yh5, '--', color='fuchsia', alpha=0.5, linewidth=1.5, label='Sn Prediction')
    if matches3:
        plt.plot(yh6, '--', color='dodgerblue', alpha=0.5, linewidth=1.5, label='Pg Prediction')
        plt.plot(yh7, '--', color='crimson', alpha=0.5, linewidth=1.5, label='Sg Prediction')
    plt.rcParams["figure.figsize"] = (16, 9)
    plt.tight_layout()
    plt.legend(loc = 'upper right', borderaxespad=0., fontsize = 16)
    plt.ylabel('Probability\n', fontsize=16)
    plt.xlabel('Sample', fontsize=16)
    fig.savefig(os.path.join(save_figs, str(evi) + '.jpg'), dpi=300)
    plt.close(fig)
    plt.clf()
def _plotter(dataset, evi, args, save_figs, yh1, yh2, yh3, yh1_std, yh2_std, yh3_std, matches):


    """

    Generates plots.

    Parameters
    ----------
    dataset: obj
        The hdf5 obj containing a NumPy array of 3 component data and associated attributes.

    evi: str
        Trace name.

    args: dic
        A dictionary containing all of the input parameters.

    save_figs: str
        Path to the folder for saving the plots.

    yh1: 1D array
        Detection probabilities.

    yh2: 1D array
        P arrival probabilities.

    yh3: 1D array
        S arrival probabilities.

    yh1_std: 1D array
        Detection standard deviations.

    yh2_std: 1D array
        P arrival standard deviations.

    yh3_std: 1D array
        S arrival standard deviations.

    matches: dic
        Contains the information for the detected and picked event.


    """


    try:
        spt = int(dataset.attrs['p_arrival_sample']);
    except Exception:
        spt = None

    try:
        sst = int(dataset.attrs['s_arrival_sample']);
    except Exception:
        sst = None

    predicted_P = []
    predicted_S = []
    if len(matches) >=1:
        for match, match_value in matches.items():
            if match_value[3]:
                predicted_P.append(match_value[3])
            else:
                predicted_P.append(None)

            if match_value[6]:
                predicted_S.append(match_value[6])
            else:
                predicted_S.append(None)


    data = np.array(dataset)

    fig = plt.figure()
    ax = fig.add_subplot(411)
    plt.plot(data[:, 0], 'k')
    plt.rcParams["figure.figsize"] = (8,5)
    legend_properties = {'weight':'bold'}
    plt.title(str(evi))
    plt.tight_layout()
    ymin, ymax = ax.get_ylim()
    pl = None
    sl = None
    ppl = None
    ssl = None

    if dataset.attrs['trace_category'] == 'earthquake_local':
        pl = plt.vlines(int(spt), ymin, ymax, color='b', linewidth=2, label='Manual_P_Arrival')
        #sl = plt.vlines(int(sst), ymin, ymax, color='r', linewidth=2, label='Manual_S_Arrival')
        # if dataset.attrs['p_status'] == 'manual':
        #     pl = plt.vlines(int(spt), ymin, ymax, color='b', linewidth=2, label='Manual_P_Arrival')
        # else:
        #     pl = plt.vlines(int(spt), ymin, ymax, color='b', linewidth=2, label='Auto_P_Arrival')

        # if dataset.attrs['s_status'] == 'manual':
        #     sl = plt.vlines(int(sst), ymin, ymax, color='r', linewidth=2, label='Manual_S_Arrival')
        # else:
        #     sl = plt.vlines(int(sst), ymin, ymax, color='r', linewidth=2, label='Auto_S_Arrival')
        # if pl or sl:
        #     plt.legend(loc = 'upper right', borderaxespad=0., prop=legend_properties)

    ax = fig.add_subplot(412)
    plt.plot(data[:, 1] , 'k')
    plt.tight_layout()
    if dataset.attrs['trace_category'] == 'earthquake_local':
        pl = plt.vlines(int(spt), ymin, ymax, color='b', linewidth=2, label='Manual_P_Arrival')
        #sl = plt.vlines(int(sst), ymin, ymax, color='r', linewidth=2, label='Manual_S_Arrival')
        # if dataset.attrs['p_status'] == 'manual':
        #     pl = plt.vlines(int(spt), ymin, ymax, color='b', linewidth=2, label='Manual_P_Arrival')
        # else:
        #     pl = plt.vlines(int(spt), ymin, ymax, color='b', linewidth=2, label='Auto_P_Arrival')

        # if dataset.attrs['s_status'] == 'manual':
        #     sl = plt.vlines(int(sst), ymin, ymax, color='r', linewidth=2, label='Manual_S_Arrival')
        # else:
        #     sl = plt.vlines(int(sst), ymin, ymax, color='r', linewidth=2, label='Auto_S_Arrival')
        if pl or sl:
            plt.legend(loc = 'upper right', borderaxespad=0., prop=legend_properties)

    ax = fig.add_subplot(413)
    plt.plot(data[:, 2], 'k')
    plt.tight_layout()
    if len(predicted_P) > 0:
        ymin, ymax = ax.get_ylim()
        for pt in predicted_P:
            if pt:
                ppl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Predicted_P_Arrival')
    if len(predicted_S) > 0:
        for st in predicted_S:
            if st:
                ssl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Predicted_S_Arrival')

    if ppl or ssl:
        plt.legend(loc = 'upper right', borderaxespad=0., prop=legend_properties)


    ax = fig.add_subplot(414)
    x = np.linspace(0, data.shape[0], data.shape[0], endpoint=True)
    if args['estimate_uncertainty']:
        plt.plot(x, yh1, 'g--', alpha = 0.5, linewidth=1.5, label='Detection')
        lowerD = yh1-yh1_std
        upperD = yh1+yh1_std
        plt.fill_between(x, lowerD, upperD, alpha=0.5, edgecolor='#3F7F4C', facecolor='#7EFF99')

        plt.plot(x, yh2, 'b--', alpha = 0.5, linewidth=1.5, label='P_probability')
        lowerP = yh2-yh2_std
        upperP = yh2+yh2_std
        plt.fill_between(x, lowerP, upperP, alpha=0.5, edgecolor='#1B2ACC', facecolor='#089FFF')

        plt.plot(x, yh3, 'r--', alpha = 0.5, linewidth=1.5, label='S_probability')
        lowerS = yh3-yh3_std
        upperS = yh3+yh3_std
        plt.fill_between(x, lowerS, upperS, edgecolor='#CC4F1B', facecolor='#FF9848')
        plt.ylim((-0.1, 1.1))
        plt.tight_layout()
        plt.legend(loc = 'upper right', borderaxespad=0., prop=legend_properties)

    else:
        plt.plot(x, yh1, 'g--', alpha = 0.5, linewidth=1.5, label='Detection')
        plt.plot(x, yh2, 'b--', alpha = 0.5, linewidth=1.5, label='P_probability')
        plt.plot(x, yh3, 'r--', alpha = 0.5, linewidth=1.5, label='S_probability')
        plt.tight_layout()
        plt.ylim((-0.1, 1.1))
        plt.legend(loc = 'upper right', borderaxespad=0., prop=legend_properties)

    fig.savefig(os.path.join(save_figs, str(evi.split('/')[-1])+'.png'))
